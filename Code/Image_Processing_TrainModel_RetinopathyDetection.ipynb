{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ///////////////////////////////////////////////////////////////////////////////\n",
    "//                   Radt Eye\n",
    "// Date:         14/11/2023\n",
    "//\n",
    "// File: Image_Processing_TrainModel_RetinopathyDetection.py\n",
    "// Description: AI model training for eye issues detection\n",
    "/////////////////////////////////////////////////////////////////////////////// \"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import os \n",
    "import tensorflow as tf \n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR) \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Avoid annoying errors\n",
    "\n",
    "batch_size = 32\n",
    "epoch = 5000\n",
    "\n",
    "def load_data(folder_path):\n",
    "    img_height = 800\n",
    "    img_width = 800\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        folder_path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        shuffle=True,\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        folder_path,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        shuffle=True,\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "def normalized_model(ds):\n",
    "    # Standarize RGB from 0 to 1\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    normalized_ds = ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "    return normalized_ds\n",
    "\n",
    "def augment_model(ds):\n",
    "    # Function for brightness adjustment\n",
    "    def adjust_brightness(image):\n",
    "        # Generate a random value for brightness adjustment between -0.2 and 0.2\n",
    "        delta = tf.random.uniform([], -0.3, 0.3)\n",
    "        return tf.image.adjust_brightness(image, delta=delta)\n",
    "\n",
    "    # Function for random contrast adjustment\n",
    "    def adjust_contrast(image):\n",
    "        # Generate a random value for contrast adjustment between 0.8 and 1.2\n",
    "        contrast_factor = tf.random.uniform([], 0.8, 1.2)\n",
    "        return tf.image.adjust_contrast(image, contrast_factor=contrast_factor)\n",
    "\n",
    "    # Data augmentation and normalization\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "        tf.keras.layers.Lambda(adjust_brightness),  # Apply brightness adjustment\n",
    "        tf.keras.layers.Lambda(adjust_contrast),    # Apply contrast adjustment\n",
    "    ])\n",
    "\n",
    "    # Apply augmentation and normalization to the dataset\n",
    "    augmented_samples = ds.map(lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "    # Concatenate the augmented samples with the original dataset\n",
    "    augmented_ds = ds.concatenate(augmented_samples)\n",
    "\n",
    "    # Configure the dataset for performance\n",
    "    augmented_ds = augmented_ds.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return normalized_model(augmented_ds)\n",
    "\n",
    "def build_model(train_normalized_ds, val_normalized_ds):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_normalized_ds = train_normalized_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_normalized_ds = val_normalized_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    num_classes = 6\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    #     0.001,\n",
    "    #     decay_steps=batch_size*1000,\n",
    "    #     decay_rate=1,\n",
    "    #     staircase=False\n",
    "    # )\n",
    "\n",
    "    # tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "folder_path = '/home/nnds3a/Documents/RadtEye/Database/2Organized'\n",
    "train_ds, val_ds = load_data(folder_path)\n",
    "\n",
    "# Class names atributtes aka health status\n",
    "# class_names = train_ds.class_names\n",
    "# print(class_names)\n",
    "\n",
    "# Show samples of the dataset\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(1):\n",
    "#   for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(class_names[labels[i]])\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "train_normalized_ds = augment_model(train_ds)\n",
    "val_normalized_ds = normalized_model(val_ds)\n",
    "\n",
    "# show\n",
    "# image_batch, labels_batch = next(iter(train_normalized_ds))\n",
    "# first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "# print(np.min(first_image), np.max(first_image))\n",
    "\n",
    "model = build_model(train_normalized_ds, val_normalized_ds)\n",
    "\n",
    "######################## Train model ######################## https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_options\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, acc = model.evaluate(image_batch, labels_batch, verbose=2)\n",
    "# print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "# # Loads the weights\n",
    "# model.load_weights(checkpoint_path)\n",
    "\n",
    "# # Re-evaluate the model\n",
    "# loss, acc = model.evaluate(image_batch, labels_batch, verbose=2)\n",
    "# print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "model_path = \"Training/MyModel.keras\"\n",
    "checkpoint_path = \"Training/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# model.load_weights(checkpoint_path) # load checkpoint\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "keras_callbacks   = [\n",
    "    ModelCheckpoint(\n",
    "        checkpoint_path, \n",
    "        monitor='val_loss', \n",
    "        verbose=1, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "model.fit(\n",
    "    train_normalized_ds,\n",
    "    validation_data=val_normalized_ds,\n",
    "    epochs=epoch,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[keras_callbacks]  # Pass callback to training\n",
    ")\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "######################## Save complet mode to H5 ########################\n",
    "# model_final = \"/Training/model_Health.h5\"\n",
    "# checkpoint_path = \"/Training/\"\n",
    "\n",
    "# model.load_weights(checkpoint_path) # load checkpoint\n",
    "\n",
    "# model.save(model_final, save_format=\"h5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
